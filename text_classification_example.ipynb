{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Towards Accurate Open-Set Recognition via Background-Class Regularization\n",
    "----\n",
    "**Wonwoo Cho and Jaegul Choo** In European Conference on Computer Vision (ECCV), 2022\n",
    "\n",
    "This notebook provides sample training and inference processes of our proposed method in a sample text classification experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(pad_first=True, lower=True, fix_length=100)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train = data.TabularDataset(path='./data/20newsgroups/20ng-train.txt',\n",
    "                                 format='csv',\n",
    "                                 fields=[('label', LABEL), ('text', TEXT)])\n",
    "\n",
    "test = data.TabularDataset(path='./data/20newsgroups/20ng-test.txt',\n",
    "                                 format='csv',\n",
    "                                 fields=[('label', LABEL), ('text', TEXT)])\n",
    "\n",
    "TEXT.build_vocab(train, max_size=10000)\n",
    "LABEL.build_vocab(train, max_size=10000)\n",
    "\n",
    "train_iter = data.BucketIterator(train, batch_size=64, repeat=False)\n",
    "test_iter = data.BucketIterator(test, batch_size=64, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length (including special tokens): 10002\n"
     ]
    }
   ],
   "source": [
    "TEXT_custom = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "custom_data = data.TabularDataset(path='./data/wikitext_reformatted/wikitext103_sentences',\n",
    "                                  format='csv',\n",
    "                                  fields=[('text', TEXT_custom)])\n",
    "\n",
    "TEXT_custom.build_vocab(train.text, max_size=10000)\n",
    "print('vocab length (including special tokens):', len(TEXT_custom.vocab))\n",
    "\n",
    "train_iter_oe = data.BucketIterator(custom_data, batch_size=64, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing model and class-wise anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2,\n",
    "            bias=True, batch_first=True,bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def initialize(self, means):\n",
    "        self.register_buffer(\"means\", nn.Parameter(means, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]\n",
    "        logits = self.linear(hidden)\n",
    "        return hidden, logits\n",
    "\n",
    "means = torch.randn((20, 128), requires_grad=True).cuda()\n",
    "model = ClfGRU(20).cuda()\n",
    "model.initialize(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer2 = torch.optim.Adam([model.means], lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer2, T_max=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_f(model):\n",
    "    model.train()\n",
    "    data_loss_ema = 0\n",
    "    oe_loss_ema = 0\n",
    "\n",
    "    for batch_idx, (batch, batch_oe) in enumerate(zip(iter(train_iter), iter(train_iter_oe))):\n",
    "        inputs = batch.text.t().cuda()\n",
    "        labels = (batch.label - 1).cuda().long()\n",
    "        feature, logits = model(inputs)\n",
    "\n",
    "        inputs_oe = batch_oe.text.t().cuda()\n",
    "        feature_oe, logits_oe = model(inputs_oe)\n",
    "        labels_oe = (torch.ones(inputs_oe.shape[0]) * 20).cuda().long()\n",
    "\n",
    "        feat = torch.cat((feature, feature_oe))\n",
    "        indlen = feature.shape[0]\n",
    "\n",
    "\n",
    "        distance = torch.norm(feat.unsqueeze(1) - model.means, p=2, dim=2)**2\n",
    "        scores = torch.igammac(torch.tensor(feat.shape[-1]/2).cuda(), distance/2)\n",
    "\n",
    "        _, index = torch.max(scores, dim=1)\n",
    "        max_scores = scores * F.one_hot(index, num_classes=20)\n",
    "        mod_scores = torch.cat((max_scores, 1-torch.sum(max_scores, dim=1).unsqueeze(1)),1)\n",
    "\n",
    "        mod_scores = torch.clamp(mod_scores, 1e-31, 1 - 1e-31)\n",
    "        oe_loss = F.nll_loss(torch.log(mod_scores), torch.cat((labels, labels_oe), dim=0))\n",
    "\n",
    "        data_loss = F.cross_entropy(-distance[:indlen]/2, labels)\n",
    "\n",
    "        loss = data_loss + 5 * oe_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer2.step()\n",
    "\n",
    "        data_loss_ema = data_loss_ema * 0.9 + data_loss.data.cpu().numpy() * 0.1\n",
    "        oe_loss_ema = oe_loss_ema * 0.9 + oe_loss.data.cpu().numpy() * 0.1\n",
    "\n",
    "        if (batch_idx % 200 == 0 or batch_idx < 10):\n",
    "            print('iter: {} \\t| data_loss_ema: {} \\t| oe_loss_ema: {}'.format(\n",
    "                batch_idx, data_loss_ema, oe_loss_ema))\n",
    "\n",
    "    scheduler.step()\n",
    "    scheduler2.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    num_examples = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(iter(test_iter)):\n",
    "        inputs = batch.text.t().cuda()\n",
    "        labels = (batch.label - 1).cuda()\n",
    "\n",
    "        feature, logits = model(inputs)\n",
    "        \n",
    "        distance = torch.norm(feature.unsqueeze(1) - model.means, p=2, dim=2)**2\n",
    "        dist_preds = torch.argmax(-distance/2, dim=1)\n",
    "            \n",
    "        acc_ind = (dist_preds == labels)\n",
    "        correct += acc_ind.sum().data.cpu().numpy()\n",
    "        num_examples += inputs.shape[0]\n",
    "\n",
    "    acc = correct / num_examples\n",
    "    loss = running_loss / num_examples\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.05167375132837407 \t| test loss: 0.0\n",
      "\n",
      "Epoch 0\n",
      "iter: 0 \t| data_loss_ema: 1.3363681793212892 \t| oe_loss_ema: 3.5393924713134766\n",
      "iter: 1 \t| data_loss_ema: 2.379087295532227 \t| oe_loss_ema: 6.45477466583252\n",
      "iter: 2 \t| data_loss_ema: 3.059560829162598 \t| oe_loss_ema: 9.216882724761964\n",
      "iter: 3 \t| data_loss_ema: 3.8100886825561524 \t| oe_loss_ema: 11.755770700454715\n",
      "iter: 4 \t| data_loss_ema: 4.389196296081543 \t| oe_loss_ema: 13.877079693031314\n",
      "iter: 5 \t| data_loss_ema: 4.707281022857666 \t| oe_loss_ema: 15.895047993259434\n",
      "iter: 6 \t| data_loss_ema: 5.024770608178711 \t| oe_loss_ema: 17.659764919397357\n",
      "iter: 7 \t| data_loss_ema: 5.378938082425538 \t| oe_loss_ema: 19.35660581027012\n",
      "iter: 8 \t| data_loss_ema: 5.5537258537728285 \t| oe_loss_ema: 20.93909021825678\n",
      "iter: 9 \t| data_loss_ema: 5.730370165597083 \t| oe_loss_ema: 22.201275556782665\n",
      "test acc: 0.6065356004250797 \t| test loss: 0.0\n",
      "\n",
      "Epoch 1\n",
      "iter: 0 \t| data_loss_ema: 0.1047799825668335 \t| oe_loss_ema: 1.0684310913085937\n",
      "iter: 1 \t| data_loss_ema: 0.19469084501266481 \t| oe_loss_ema: 1.7520686435699462\n",
      "iter: 2 \t| data_loss_ema: 0.26741218113899234 \t| oe_loss_ema: 2.5361812753677366\n",
      "iter: 3 \t| data_loss_ema: 0.3171916801691056 \t| oe_loss_ema: 2.852510533618927\n",
      "iter: 4 \t| data_loss_ema: 0.37234675278902063 \t| oe_loss_ema: 3.4195379035663604\n",
      "iter: 5 \t| data_loss_ema: 0.4425932591316701 \t| oe_loss_ema: 3.9796711340837483\n",
      "iter: 6 \t| data_loss_ema: 0.46992637739994536 \t| oe_loss_ema: 4.372768235381795\n",
      "iter: 7 \t| data_loss_ema: 0.5108444633461691 \t| oe_loss_ema: 4.836992876687366\n",
      "iter: 8 \t| data_loss_ema: 0.5529457159976057 \t| oe_loss_ema: 5.419606474303054\n",
      "iter: 9 \t| data_loss_ema: 0.5723906244164608 \t| oe_loss_ema: 5.667578077849311\n",
      "test acc: 0.695403825717322 \t| test loss: 0.0\n",
      "\n",
      "Epoch 2\n",
      "iter: 0 \t| data_loss_ema: 0.037068146467208865 \t| oe_loss_ema: 0.2839573621749878\n",
      "iter: 1 \t| data_loss_ema: 0.057217704951763156 \t| oe_loss_ema: 0.37206323146820075\n",
      "iter: 2 \t| data_loss_ema: 0.09869687351584436 \t| oe_loss_ema: 0.7287919211387636\n",
      "iter: 3 \t| data_loss_ema: 0.1102970608443022 \t| oe_loss_ema: 0.9393734891414643\n",
      "iter: 4 \t| data_loss_ema: 0.12874654248684647 \t| oe_loss_ema: 1.1294784271001816\n",
      "iter: 5 \t| data_loss_ema: 0.1304131580769718 \t| oe_loss_ema: 1.2462395659041405\n",
      "iter: 6 \t| data_loss_ema: 0.1303236484569043 \t| oe_loss_ema: 1.350772794868231\n",
      "iter: 7 \t| data_loss_ema: 0.14735535383597584 \t| oe_loss_ema: 1.6661860854497674\n",
      "iter: 8 \t| data_loss_ema: 0.15493231058456514 \t| oe_loss_ema: 1.72935971418659\n",
      "iter: 9 \t| data_loss_ema: 0.16333571261469734 \t| oe_loss_ema: 1.7277155113188039\n",
      "test acc: 0.7232996811902231 \t| test loss: 0.0\n",
      "\n",
      "Epoch 3\n",
      "iter: 0 \t| data_loss_ema: 0.005525172501802445 \t| oe_loss_ema: 0.11374546289443971\n",
      "iter: 1 \t| data_loss_ema: 0.02014995135366917 \t| oe_loss_ema: 0.21618592619895938\n",
      "iter: 2 \t| data_loss_ema: 0.018437995033338667 \t| oe_loss_ema: 0.19640902248024944\n",
      "iter: 3 \t| data_loss_ema: 0.02453944401461631 \t| oe_loss_ema: 0.23524966448843482\n",
      "iter: 4 \t| data_loss_ema: 0.03848491594063118 \t| oe_loss_ema: 0.3256688112941385\n",
      "iter: 5 \t| data_loss_ema: 0.05533934520065971 \t| oe_loss_ema: 0.4622816915744842\n",
      "iter: 6 \t| data_loss_ema: 0.05175131513667275 \t| oe_loss_ema: 0.4204957042277724\n",
      "iter: 7 \t| data_loss_ema: 0.05266423892073783 \t| oe_loss_ema: 0.49372317380972536\n",
      "iter: 8 \t| data_loss_ema: 0.06232253197812694 \t| oe_loss_ema: 0.5593281446749077\n",
      "iter: 9 \t| data_loss_ema: 0.07089037284836464 \t| oe_loss_ema: 0.6734656326938366\n",
      "test acc: 0.7308714133900106 \t| test loss: 0.0\n",
      "\n",
      "Epoch 4\n",
      "iter: 0 \t| data_loss_ema: 0.024229928851127625 \t| oe_loss_ema: 0.1691352128982544\n",
      "iter: 1 \t| data_loss_ema: 0.027839700505137444 \t| oe_loss_ema: 0.26523445844650273\n",
      "iter: 2 \t| data_loss_ema: 0.02748662002384663 \t| oe_loss_ema: 0.2973643434047699\n",
      "iter: 3 \t| data_loss_ema: 0.02566357577964664 \t| oe_loss_ema: 0.2694257892966271\n",
      "iter: 4 \t| data_loss_ema: 0.023715712177753454 \t| oe_loss_ema: 0.24442947524487976\n",
      "iter: 5 \t| data_loss_ema: 0.05823905248761178 \t| oe_loss_ema: 0.3892144854459167\n",
      "iter: 6 \t| data_loss_ema: 0.06432319712030889 \t| oe_loss_ema: 0.41014628668960934\n",
      "iter: 7 \t| data_loss_ema: 0.0595492961828828 \t| oe_loss_ema: 0.3715061811536879\n",
      "iter: 8 \t| data_loss_ema: 0.06106788302882792 \t| oe_loss_ema: 0.44765240561339237\n",
      "iter: 9 \t| data_loss_ema: 0.06127723949036851 \t| oe_loss_ema: 0.5165260309997766\n",
      "test acc: 0.7395058448459086 \t| test loss: 0.0\n",
      "\n",
      "Epoch 5\n",
      "iter: 0 \t| data_loss_ema: 0.0031483475118875507 \t| oe_loss_ema: 0.05676043629646302\n",
      "iter: 1 \t| data_loss_ema: 0.004297024998813868 \t| oe_loss_ema: 0.05299288831651212\n",
      "iter: 2 \t| data_loss_ema: 0.007072730282321573 \t| oe_loss_ema: 0.10509073855727913\n",
      "iter: 3 \t| data_loss_ema: 0.010627319368161262 \t| oe_loss_ema: 0.20791674961671236\n",
      "iter: 4 \t| data_loss_ema: 0.02377426322022453 \t| oe_loss_ema: 0.299687731057629\n",
      "iter: 5 \t| data_loss_ema: 0.0281875225518886 \t| oe_loss_ema: 0.38230657677968594\n",
      "iter: 6 \t| data_loss_ema: 0.03350203194750045 \t| oe_loss_ema: 0.40101873369026414\n",
      "iter: 7 \t| data_loss_ema: 0.04433497749447251 \t| oe_loss_ema: 0.474661107280924\n",
      "iter: 8 \t| data_loss_ema: 0.04169198949050389 \t| oe_loss_ema: 0.42943964135117\n",
      "iter: 9 \t| data_loss_ema: 0.03765355348126054 \t| oe_loss_ema: 0.3876609864850343\n",
      "test acc: 0.7412327311370882 \t| test loss: 0.0\n",
      "\n",
      "Epoch 6\n",
      "iter: 0 \t| data_loss_ema: 0.007651771605014801 \t| oe_loss_ema: 0.057122337818145755\n",
      "iter: 1 \t| data_loss_ema: 0.013608561903238298 \t| oe_loss_ema: 0.10812256455421448\n",
      "iter: 2 \t| data_loss_ema: 0.018660567209124568 \t| oe_loss_ema: 0.21044886457920076\n",
      "iter: 3 \t| data_loss_ema: 0.02203447699993849 \t| oe_loss_ema: 0.30195637060403824\n",
      "iter: 4 \t| data_loss_ema: 0.02242741228297353 \t| oe_loss_ema: 0.3287694708693028\n",
      "iter: 5 \t| data_loss_ema: 0.029387957523658872 \t| oe_loss_ema: 0.3523997059036494\n",
      "iter: 6 \t| data_loss_ema: 0.026725331557522717 \t| oe_loss_ema: 0.3200730358200551\n",
      "iter: 7 \t| data_loss_ema: 0.04444885070270047 \t| oe_loss_ema: 0.3453527994129611\n",
      "iter: 8 \t| data_loss_ema: 0.04161727095525633 \t| oe_loss_ema: 0.36793386106254633\n",
      "iter: 9 \t| data_loss_ema: 0.046020089073906874 \t| oe_loss_ema: 0.3885930046250997\n",
      "test acc: 0.7436238044633369 \t| test loss: 0.0\n",
      "\n",
      "Epoch 7\n",
      "iter: 0 \t| data_loss_ema: 1.7925900465343148e-05 \t| oe_loss_ema: 0.0008999450132250787\n",
      "iter: 1 \t| data_loss_ema: 1.8925997756014114e-05 \t| oe_loss_ema: 0.0014744766894727946\n",
      "iter: 2 \t| data_loss_ema: 6.078973523472087e-05 \t| oe_loss_ema: 0.0029189100125804543\n",
      "iter: 3 \t| data_loss_ema: 0.0001883036765011639 \t| oe_loss_ema: 0.0033508300640620295\n",
      "iter: 4 \t| data_loss_ema: 0.00017573616576501083 \t| oe_loss_ema: 0.004332155684391038\n",
      "iter: 5 \t| data_loss_ema: 0.006601617619445697 \t| oe_loss_ema: 0.11619781020262186\n",
      "iter: 6 \t| data_loss_ema: 0.01149220918272505 \t| oe_loss_ema: 0.16121333744804572\n",
      "iter: 7 \t| data_loss_ema: 0.017446023118387787 \t| oe_loss_ema: 0.2572460464240358\n",
      "iter: 8 \t| data_loss_ema: 0.01572779328326274 \t| oe_loss_ema: 0.2319454927870255\n",
      "iter: 9 \t| data_loss_ema: 0.015432168639003239 \t| oe_loss_ema: 0.20978540391134812\n",
      "test acc: 0.7434909670563231 \t| test loss: 0.0\n",
      "\n",
      "Epoch 8\n",
      "iter: 0 \t| data_loss_ema: 9.63749844231643e-06 \t| oe_loss_ema: 0.00023830183781683447\n",
      "iter: 1 \t| data_loss_ema: 1.4260103343985976e-05 \t| oe_loss_ema: 0.0004607174405828119\n",
      "iter: 2 \t| data_loss_ema: 4.015559033723548e-05 \t| oe_loss_ema: 0.0011845293701626361\n",
      "iter: 3 \t| data_loss_ema: 3.9404609421035274e-05 \t| oe_loss_ema: 0.00130825045411475\n",
      "iter: 4 \t| data_loss_ema: 3.814338253665483e-05 \t| oe_loss_ema: 0.0015406418414460497\n",
      "iter: 5 \t| data_loss_ema: 3.462165887829906e-05 \t| oe_loss_ema: 0.0020907568492633294\n",
      "iter: 6 \t| data_loss_ema: 5.458228339068738e-05 \t| oe_loss_ema: 0.002187257344444888\n",
      "iter: 7 \t| data_loss_ema: 5.219216521505864e-05 \t| oe_loss_ema: 0.002499144311550526\n",
      "iter: 8 \t| data_loss_ema: 0.00010677913749485628 \t| oe_loss_ema: 0.0027366939879685057\n",
      "iter: 9 \t| data_loss_ema: 9.747348845079439e-05 \t| oe_loss_ema: 0.002794978502145059\n",
      "test acc: 0.751461211477152 \t| test loss: 0.0\n",
      "\n",
      "Epoch 9\n",
      "iter: 0 \t| data_loss_ema: 1.4904834097251296e-05 \t| oe_loss_ema: 0.00025705224834382534\n",
      "iter: 1 \t| data_loss_ema: 0.000141497619333677 \t| oe_loss_ema: 0.0005153198027983308\n",
      "iter: 2 \t| data_loss_ema: 0.015419168323918712 \t| oe_loss_ema: 0.11214212587336078\n",
      "iter: 3 \t| data_loss_ema: 0.013877963682539848 \t| oe_loss_ema: 0.10148426848859526\n",
      "iter: 4 \t| data_loss_ema: 0.0124969051595372 \t| oe_loss_ema: 0.09188017179148737\n",
      "iter: 5 \t| data_loss_ema: 0.011247866159840247 \t| oe_loss_ema: 0.08287463451171527\n",
      "iter: 6 \t| data_loss_ema: 0.010127495100251765 \t| oe_loss_ema: 0.07571132509182045\n",
      "iter: 7 \t| data_loss_ema: 0.009115813722268426 \t| oe_loss_ema: 0.0686365105876638\n",
      "iter: 8 \t| data_loss_ema: 0.008221768595328048 \t| oe_loss_ema: 0.06196588498005094\n",
      "iter: 9 \t| data_loss_ema: 0.0074114610200031765 \t| oe_loss_ema: 0.05599501861988445\n",
      "test acc: 0.7545164718384697 \t| test loss: 0.0\n",
      "\n",
      "Epoch 10\n",
      "iter: 0 \t| data_loss_ema: 2.8330337045190393e-07 \t| oe_loss_ema: 9.704718249849976e-05\n",
      "iter: 1 \t| data_loss_ema: 8.124248461172102e-07 \t| oe_loss_ema: 0.00019897466467227787\n",
      "iter: 2 \t| data_loss_ema: 2.5252312032989722e-05 \t| oe_loss_ema: 0.0005793934943503701\n",
      "iter: 3 \t| data_loss_ema: 4.549642911811134e-05 \t| oe_loss_ema: 0.0007291006154322532\n",
      "iter: 4 \t| data_loss_ema: 0.007271375392478045 \t| oe_loss_ema: 0.057420823017344226\n",
      "iter: 5 \t| data_loss_ema: 0.006546439080701473 \t| oe_loss_ema: 0.051775312225103315\n",
      "iter: 6 \t| data_loss_ema: 0.005894189848170434 \t| oe_loss_ema: 0.046839444286261446\n",
      "iter: 7 \t| data_loss_ema: 0.0053064948784089065 \t| oe_loss_ema: 0.04285781933811406\n",
      "iter: 8 \t| data_loss_ema: 0.004788229186842678 \t| oe_loss_ema: 0.03875925163542122\n",
      "iter: 9 \t| data_loss_ema: 0.004309605941171823 \t| oe_loss_ema: 0.03502433331972263\n",
      "test acc: 0.754383634431456 \t| test loss: 0.0\n",
      "\n",
      "Epoch 11\n",
      "iter: 0 \t| data_loss_ema: 4.787933721672744e-06 \t| oe_loss_ema: 0.00012084189802408218\n",
      "iter: 1 \t| data_loss_ema: 1.4422952808672563e-05 \t| oe_loss_ema: 0.0002343860687687993\n",
      "iter: 2 \t| data_loss_ema: 1.619900530204177e-05 \t| oe_loss_ema: 0.000421233952511102\n",
      "iter: 3 \t| data_loss_ema: 1.558430232689716e-05 \t| oe_loss_ema: 0.0005078631441574544\n",
      "iter: 4 \t| data_loss_ema: 1.4308802890154768e-05 \t| oe_loss_ema: 0.0005351356604369358\n",
      "iter: 5 \t| data_loss_ema: 1.3937084023884383e-05 \t| oe_loss_ema: 0.0006225779872504063\n",
      "iter: 6 \t| data_loss_ema: 2.3786147491952583e-05 \t| oe_loss_ema: 0.0007323918247141409\n",
      "iter: 7 \t| data_loss_ema: 2.1478685298604546e-05 \t| oe_loss_ema: 0.0007483864998222561\n",
      "iter: 8 \t| data_loss_ema: 1.9628083065655495e-05 \t| oe_loss_ema: 0.000758799302241311\n",
      "iter: 9 \t| data_loss_ema: 2.1986749020904673e-05 \t| oe_loss_ema: 0.0008022824345665624\n",
      "test acc: 0.7542507970244421 \t| test loss: 0.0\n",
      "\n",
      "Epoch 12\n",
      "iter: 0 \t| data_loss_ema: 1.0479010234121235e-05 \t| oe_loss_ema: 0.00010325753828510642\n",
      "iter: 1 \t| data_loss_ema: 1.0601963222143242e-05 \t| oe_loss_ema: 0.00016339161666110159\n",
      "iter: 2 \t| data_loss_ema: 9.64141728991308e-06 \t| oe_loss_ema: 0.00023305369773879647\n",
      "iter: 3 \t| data_loss_ema: 9.579063386490815e-06 \t| oe_loss_ema: 0.00027830794197507204\n",
      "iter: 4 \t| data_loss_ema: 8.722110971107212e-06 \t| oe_loss_ema: 0.000395856531825848\n",
      "iter: 5 \t| data_loss_ema: 9.81969441303272e-06 \t| oe_loss_ema: 0.0004322495305188932\n",
      "iter: 6 \t| data_loss_ema: 9.338931585719229e-06 \t| oe_loss_ema: 0.0004810403305015061\n",
      "iter: 7 \t| data_loss_ema: 8.6050810592065e-06 \t| oe_loss_ema: 0.0006894834895629668\n",
      "iter: 8 \t| data_loss_ema: 1.810096147373525e-05 \t| oe_loss_ema: 0.0007086958060796395\n",
      "iter: 9 \t| data_loss_ema: 1.685107236232177e-05 \t| oe_loss_ema: 0.0006879753675153406\n",
      "test acc: 0.7533209351753454 \t| test loss: 0.0\n",
      "\n",
      "Epoch 13\n",
      "iter: 0 \t| data_loss_ema: 1.7713442730382669e-07 \t| oe_loss_ema: 6.47593813482672e-05\n",
      "iter: 1 \t| data_loss_ema: 1.8067234361751619e-06 \t| oe_loss_ema: 0.0001442952622892335\n",
      "iter: 2 \t| data_loss_ema: 1.960202356372065e-06 \t| oe_loss_ema: 0.00026882855204166844\n",
      "iter: 3 \t| data_loss_ema: 3.0205530085936517e-06 \t| oe_loss_ema: 0.0003316254542034585\n",
      "iter: 4 \t| data_loss_ema: 3.0347500476057123e-06 \t| oe_loss_ema: 0.0003713114727969514\n",
      "iter: 5 \t| data_loss_ema: 3.0242575514990903e-06 \t| oe_loss_ema: 0.0004079373714195681\n",
      "iter: 6 \t| data_loss_ema: 3.3496747700486252e-06 \t| oe_loss_ema: 0.0004283678781140887\n",
      "iter: 7 \t| data_loss_ema: 5.277106468422722e-06 \t| oe_loss_ema: 0.000506640871623818\n",
      "iter: 8 \t| data_loss_ema: 5.234042947211905e-06 \t| oe_loss_ema: 0.0005522802011103158\n",
      "iter: 9 \t| data_loss_ema: 8.103448974602186e-06 \t| oe_loss_ema: 0.0006126979742658786\n",
      "test acc: 0.7525239107332625 \t| test loss: 0.0\n",
      "\n",
      "Epoch 14\n",
      "iter: 0 \t| data_loss_ema: 1.3958593626739458e-06 \t| oe_loss_ema: 6.13838667050004e-05\n",
      "iter: 1 \t| data_loss_ema: 1.6349370434909363e-06 \t| oe_loss_ema: 0.00013070133398287\n",
      "iter: 2 \t| data_loss_ema: 3.895205428307236e-06 \t| oe_loss_ema: 0.0005013849259121344\n",
      "iter: 3 \t| data_loss_ema: 4.1415484655772165e-06 \t| oe_loss_ema: 0.0005301697059883737\n",
      "iter: 4 \t| data_loss_ema: 5.484322014499413e-06 \t| oe_loss_ema: 0.0005676272485160735\n",
      "iter: 5 \t| data_loss_ema: 5.305793836550039e-06 \t| oe_loss_ema: 0.0005928892402037746\n",
      "iter: 6 \t| data_loss_ema: 5.143069021034627e-06 \t| oe_loss_ema: 0.0006293866800011485\n",
      "iter: 7 \t| data_loss_ema: 5.521087470673493e-06 \t| oe_loss_ema: 0.0006722103889166971\n",
      "iter: 8 \t| data_loss_ema: 5.0554045604044425e-06 \t| oe_loss_ema: 0.0007168472099084849\n",
      "iter: 9 \t| data_loss_ema: 5.017351293235017e-06 \t| oe_loss_ema: 0.0007161520457610514\n",
      "test acc: 0.7530552603613178 \t| test loss: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('test acc: {} \\t| test loss: {}\\n'.format(acc, loss))\n",
    "for epoch in range(15):\n",
    "    print('Epoch', epoch)\n",
    "    train_f(model)\n",
    "    acc, loss = evaluate(model)\n",
    "    print('test acc: {} \\t| test loss: {}\\n'.format(acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unknown-Class Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length (including special tokens): 10002\n"
     ]
    }
   ],
   "source": [
    "TEXT_20ng = data.Field(pad_first=True, lower=True, fix_length=100)\n",
    "LABEL_20ng = data.Field(sequential=False)\n",
    "\n",
    "train_20ng = data.TabularDataset(path='./data/20newsgroups/20ng-train.txt',\n",
    "                                 format='csv',\n",
    "                                 fields=[('label', LABEL_20ng), ('text', TEXT_20ng)])\n",
    "\n",
    "test_20ng = data.TabularDataset(path='./data/20newsgroups/20ng-test.txt',\n",
    "                                 format='csv',\n",
    "                                 fields=[('label', LABEL_20ng), ('text', TEXT_20ng)])\n",
    "\n",
    "TEXT_20ng.build_vocab(train_20ng, max_size=10000)\n",
    "LABEL_20ng.build_vocab(train_20ng, max_size=10000)\n",
    "print('vocab length (including special tokens):', len(TEXT_20ng.vocab))\n",
    "\n",
    "train_iter_20ng = data.BucketIterator(train_20ng, batch_size=64, repeat=False)\n",
    "test_iter_20ng = data.BucketIterator(test_20ng, batch_size=64, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length (including special tokens): 10002\n"
     ]
    }
   ],
   "source": [
    "TEXT_m30k = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "m30k_data = data.TabularDataset(path='./data/multi30k/train.txt',\n",
    "                                  format='csv',\n",
    "                                  fields=[('text', TEXT_m30k)])\n",
    "\n",
    "TEXT_m30k.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length (including special tokens):', len(TEXT_m30k.vocab))\n",
    "\n",
    "train_iter_m30k = data.BucketIterator(m30k_data, batch_size=64, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length (including special tokens): 10002\n"
     ]
    }
   ],
   "source": [
    "TEXT_wmt16 = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "wmt16_data = data.TabularDataset(path='./data/wmt16/wmt16_sentences',\n",
    "                                  format='csv',\n",
    "                                  fields=[('text', TEXT_wmt16)])\n",
    "\n",
    "TEXT_wmt16.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length (including special tokens):', len(TEXT_wmt16.vocab))\n",
    "\n",
    "train_iter_wmt16 = data.BucketIterator(wmt16_data, batch_size=64, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length (including special tokens): 10002\n"
     ]
    }
   ],
   "source": [
    "# set up fields\n",
    "TEXT_imdb = data.Field(pad_first=True, lower=True)\n",
    "LABEL_imdb = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_imdb, test_imdb = datasets.IMDB.splits(TEXT_imdb, LABEL_imdb)\n",
    "\n",
    "# build vocab\n",
    "TEXT_imdb.build_vocab(train_20ng.text, max_size=10000)\n",
    "LABEL_imdb.build_vocab(train_imdb, max_size=10000)\n",
    "print('vocab length (including special tokens):', len(TEXT_imdb.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make iterators\n",
    "train_iter_imdb, test_iter_imdb = data.BucketIterator.splits(\n",
    "    (train_imdb, test_imdb), batch_size=64, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPR95 Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_cumsum(arr, rtol=1e-05, atol=1e-08):\n",
    "    \"\"\"Use high precision for cumsum and check that final value matches sum\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "        To be cumulatively summed as flat\n",
    "    rtol : float\n",
    "        Relative tolerance, see ``np.allclose``\n",
    "    atol : float\n",
    "        Absolute tolerance, see ``np.allclose``\n",
    "    \"\"\"\n",
    "    out = np.cumsum(arr, dtype=np.float64)\n",
    "    expected = np.sum(arr, dtype=np.float64)\n",
    "    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):\n",
    "        raise RuntimeError('cumsum was found to be unstable: '\n",
    "                           'its last element does not correspond to sum')\n",
    "    return out\n",
    "\n",
    "def fpr_and_fdr_at_recall(y_true, y_score, recall_level=0.95, pos_label=None):\n",
    "    classes = np.unique(y_true)\n",
    "    if (pos_label is None and\n",
    "            not (np.array_equal(classes, [0, 1]) or\n",
    "                     np.array_equal(classes, [-1, 1]) or\n",
    "                     np.array_equal(classes, [0]) or\n",
    "                     np.array_equal(classes, [-1]) or\n",
    "                     np.array_equal(classes, [1]))):\n",
    "        raise ValueError(\"Data is not binary and pos_label is not specified\")\n",
    "    elif pos_label is None:\n",
    "        pos_label = 1.\n",
    "\n",
    "    # make y_true a boolean vector\n",
    "    y_true = (y_true == pos_label)\n",
    "\n",
    "    # sort scores and corresponding truth values\n",
    "    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n",
    "    y_score = y_score[desc_score_indices]\n",
    "    y_true = y_true[desc_score_indices]\n",
    "\n",
    "    # y_score typically has many tied values. Here we extract\n",
    "    # the indices associated with the distinct values. We also\n",
    "    # concatenate a value for the end of the curve.\n",
    "    distinct_value_indices = np.where(np.diff(y_score))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n",
    "\n",
    "    # accumulate the true positives with decreasing threshold\n",
    "    tps = stable_cumsum(y_true)[threshold_idxs]\n",
    "    fps = 1 + threshold_idxs - tps      # add one because of zero-based indexing\n",
    "\n",
    "    thresholds = y_score[threshold_idxs]\n",
    "\n",
    "    recall = tps / tps[-1]\n",
    "\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)      # [last_ind::-1]\n",
    "    recall, fps, tps, thresholds = np.r_[recall[sl], 1], np.r_[fps[sl], 0], np.r_[tps[sl], 0], thresholds[sl]\n",
    "\n",
    "    cutoff = np.argmin(np.abs(recall - recall_level))\n",
    "\n",
    "    return fps[cutoff] / (np.sum(np.logical_not(y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "true = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(iter(test_iter)):\n",
    "        inputs = batch.text.t().cuda()\n",
    "        labels = (batch.label - 1).cuda()\n",
    "\n",
    "        feature, logits = model(inputs)\n",
    "\n",
    "        distance = torch.norm(feature.unsqueeze(1) - model.means, p=2, dim=2)**2       \n",
    "        min_dist, _ = torch.min(distance, dim=1)\n",
    "            \n",
    "        logits_all = -distance/2\n",
    "        preds_all = torch.argmax(logits_all, dim=1)\n",
    "            \n",
    "        y_true = np.append(y_true, labels.detach().cpu().numpy())\n",
    "        y_pred = np.append(y_pred, preds_all.detach().cpu().numpy())\n",
    "\n",
    "        score = np.append(score, min_dist.detach().cpu().numpy())\n",
    "        true = np.append(true, np.zeros_like(min_dist.detach().cpu().numpy()))\n",
    "        \n",
    "        if batch_idx >= 116:\n",
    "            break\n",
    "            \n",
    "    for batch_idx, batch in enumerate(iter(train_iter_imdb)):\n",
    "        inputs = batch.text.t().cuda()\n",
    "        feature, logits = model(inputs)\n",
    "\n",
    "        distance = torch.norm(feature.unsqueeze(1) - model.means, p=2, dim=2)**2       \n",
    "        min_dist, _ = torch.min(distance, dim=1)\n",
    "            \n",
    "        logits_all = -distance/2\n",
    "        preds_all = torch.argmax(logits_all, dim=1)\n",
    "        \n",
    "        y_true = np.append(y_true, labels.detach().cpu().numpy())\n",
    "        y_pred = np.append(y_pred, preds_all.detach().cpu().numpy())\n",
    "        \n",
    "        score = np.append(score, min_dist.detach().cpu().numpy())\n",
    "        true = np.append(true, np.ones_like(min_dist.detach().cpu().numpy()))\n",
    "        \n",
    "        if batch_idx >= 116:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.9990908282834529\n",
      "AUPR: 0.9990259424232494\n",
      "FPR95: 0.00334941050375134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "print(\"AUROC:\", roc_auc_score(true, score))\n",
    "print(\"AUPR:\", average_precision_score(true, score))\n",
    "print(\"FPR95:\",fpr_and_fdr_at_recall(true, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.2787353515625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10008038585209003"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "threshold = np.percentile(score[64*117:], 10)\n",
    "print(threshold)\n",
    "\n",
    "(score[64*117:] < threshold).sum() / len(score[64*117:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750801282051282\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "index = np.where(score > threshold)\n",
    "y_pred2 = copy.deepcopy(y_pred)\n",
    "y_pred2[index] = 20\n",
    "\n",
    "print((y_true[:64*117] == y_pred2[:64*117]).sum()/(64*117))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wonwoo2",
   "language": "python",
   "name": "wonwoo2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
